{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os                # 用于操作系统相关的操作，例如读取环境变量\n",
    "import io                # 用于处理流式数据（例如文件流）\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llm.call_llm import get_completion\n",
    "from database.create_db import create_db_info\n",
    "from qa_chain.Chat_QA_chain_self import Chat_QA_chain_self\n",
    "from qa_chain.QA_chain_self import QA_chain_self\n",
    "import re\n",
    "from qa_chain import get_vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_DICT = {\n",
    "    \"wenxin\": [\"ERNIE-Speed-128K\"],\n",
    "}\n",
    "LLM_MODEL_LIST = sum(list(LLM_MODEL_DICT.values()),[])\n",
    "INIT_EMBEDDING_MODEL = \"m3e\"\n",
    "DEFAULT_DB_PATH = \"knowledge_db\"\n",
    "DEFAULT_PERSIST_PATH = \"vector_db/chroma\"\n",
    "api_key=\"you api\"\n",
    "Wenxin_secret_key=\"you secret key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于构建自己的知识库\n",
    "如果你想使用自己的知识库，请删除vector_db中的文件，然后运行下面代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_vectordb.get_vectordb(file_path=\"knowledge\",persist_path=\"vector_db\",embedding=\"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_center():\n",
    "    \"\"\"\n",
    "    存储问答 Chain 的对象 \n",
    "\n",
    "    - chat_qa_chain_self: 以 (model, embedding) 为键存储的带历史记录的问答链。\n",
    "    - qa_chain_self: 以 (model, embedding) 为键存储的不带历史记录的问答链。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.chat_qa_chain_self = {}\n",
    "        self.qa_chain_self = {}\n",
    "\n",
    "    def chat_qa_chain_self_answer(self, question: str, chat_history: list = [], model: str = \"ERNIE-Speed-128K\", embedding: str = \"m3e\", temperature: float = 0.0, top_k: int = 4, history_len: int = 3, file_path: str = DEFAULT_DB_PATH, persist_path: str = DEFAULT_PERSIST_PATH):\n",
    "        \"\"\"\n",
    "        调用带历史记录的问答链进行回答\n",
    "        \"\"\"\n",
    "        if question == None or len(question) < 1:\n",
    "            return \"\", chat_history\n",
    "        try:\n",
    "            if (model, embedding) not in self.chat_qa_chain_self:\n",
    "                self.chat_qa_chain_self[(model, embedding)] = Chat_QA_chain_self(model=model, temperature=temperature,\n",
    "                                                                                    top_k=top_k, chat_history=chat_history, file_path=file_path, persist_path=persist_path, embedding=embedding)\n",
    "            chain = self.chat_qa_chain_self[(model, embedding)]\n",
    "            answer,source_documents=chain.answer(question, temperature, top_k)\n",
    "            chat_history.append(\n",
    "                (question, answer))\n",
    "            return chat_history,source_documents\n",
    "        except Exception as e:\n",
    "            return e, chat_history\n",
    "\n",
    "    def qa_chain_self_answer(self, question: str, chat_history: list = [], model: str = \"ERNIE-Speed-128K\", embedding=\"m3e\", temperature: float = 0.0, top_k: int = 4, file_path: str = DEFAULT_DB_PATH, persist_path: str = DEFAULT_PERSIST_PATH):\n",
    "        \"\"\"\n",
    "        调用不带历史记录的问答链进行回答\n",
    "        \"\"\"\n",
    "        if question == None or len(question) < 1:\n",
    "            return \"\", chat_history\n",
    "        try:\n",
    "            if (model, embedding) not in self.qa_chain_self:\n",
    "                self.qa_chain_self[(model, embedding)] = QA_chain_self(model=model, temperature=temperature,\n",
    "                                                                       top_k=top_k, file_path=file_path, persist_path=persist_path, embedding=embedding,api_key=api_key,Wenxin_secret_key=Wenxin_secret_key)\n",
    "            chain = self.qa_chain_self[(model, embedding)]\n",
    "            answer,source_documents=chain.answer(question, temperature, top_k)\n",
    "            chat_history.append(\n",
    "                (question, answer))\n",
    "            return \"\", chat_history,source_documents\n",
    "        except Exception as e:\n",
    "            return e, chat_history\n",
    "\n",
    "    def clear_history(self):\n",
    "        if len(self.chat_qa_chain_self) > 0:\n",
    "            for chain in self.chat_qa_chain_self.values():\n",
    "                chain.clear_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    \"\"\"\n",
    "    该函数用于格式化聊天 prompt。\n",
    "\n",
    "    参数:\n",
    "    message: 当前的用户消息。\n",
    "    chat_history: 聊天历史记录。\n",
    "\n",
    "    返回:\n",
    "    prompt: 格式化后的 prompt。\n",
    "    \"\"\"\n",
    "    # 初始化一个空字符串，用于存放格式化后的聊天 prompt。\n",
    "    prompt = \"\"\n",
    "    # 遍历聊天历史记录。\n",
    "    for turn in chat_history:\n",
    "        # 从聊天记录中提取用户和机器人的消息。\n",
    "        user_message, bot_message = turn\n",
    "        # 更新 prompt，加入用户和机器人的消息。\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    # 将当前的用户消息也加入到 prompt中，并预留一个位置给机器人的回复。\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    # 返回格式化后的 prompt。\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(message, chat_history, llm, history_len=3, temperature=0.1, max_tokens=2048):\n",
    "    \"\"\"\n",
    "    该函数用于生成机器人的回复。\n",
    "\n",
    "    参数:\n",
    "    message: 当前的用户消息。\n",
    "    chat_history: 聊天历史记录。\n",
    "\n",
    "    返回:\n",
    "    \"\": 空字符串表示没有内容需要显示在界面上，可以替换为真正的机器人回复。\n",
    "    chat_history: 更新后的聊天历史记录\n",
    "    \"\"\"\n",
    "    if message == None or len(message) < 1:\n",
    "            return \"\", chat_history\n",
    "    try:\n",
    "        # 限制 history 的记忆长度\n",
    "        chat_history = chat_history[-history_len:] if history_len > 0 else []\n",
    "        # 调用上面的函数，将用户的消息和聊天历史记录格式化为一个 prompt。\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        # 使用llm对象的predict方法生成机器人的回复（注意：llm对象在此代码中并未定义）。\n",
    "        bot_message = get_completion(\n",
    "            formatted_prompt, llm, temperature=temperature, max_tokens=max_tokens)\n",
    "        # 将bot_message中\\n换为<br/>\n",
    "        bot_message = re.sub(r\"\\\\n\", '<br/>', bot_message)\n",
    "        print(bot_message)\n",
    "        # 将用户的消息和机器人的回复加入到聊天历史记录中。\n",
    "        chat_history.append((message, bot_message))\n",
    "        # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n",
    "        return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        return e, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_center = Model_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history,source_documents=model_center.chat_qa_chain_self_answer(question=\"输入客户的评论文本和对应的情感分析结果\",temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeyError('source_documents')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m source_documents:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'metadata'"
     ]
    }
   ],
   "source": [
    "for doc in source_documents:\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImportError('Could not import chromadb python package. Please install it with `pip install chromadb`.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
